{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 4.1: Data Cleaning & Transformation\n",
    "\n",
    "**Durasi:** 60 menit  \n",
    "**Dataset:** RUP 2025\n",
    "\n",
    "## Tujuan Pembelajaran\n",
    "- Mendeteksi dan menangani missing values\n",
    "- Mendeteksi dan menangani outliers\n",
    "- Encoding categorical variables\n",
    "- Feature engineering\n",
    "- Data transformation techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"✅ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_path = Path('../../../datasets/rup/RUP-PaketPenyedia-Terumumkan-2025.parquet')\n",
    "df_original = pd.read_parquet(data_path)\n",
    "\n",
    "# Create working copy\n",
    "df = df_original.copy()\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Quality Assessment\n",
    "\n",
    "### 3.1 Check Data Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic info\n",
    "print(\"=\" * 50)\n",
    "print(\"DATA INFO\")\n",
    "print(\"=\" * 50)\n",
    "df.info()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"COLUMN NAMES\")\n",
    "print(\"=\" * 50)\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"{i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Missing Values Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate missing values\n",
    "missing = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Missing_Count': df.isnull().sum().values,\n",
    "    'Missing_Percentage': (df.isnull().sum().values / len(df) * 100).round(2),\n",
    "    'Dtype': df.dtypes.values\n",
    "})\n",
    "\n",
    "missing = missing[missing['Missing_Count'] > 0].sort_values('Missing_Percentage', ascending=False)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MISSING VALUES SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(missing.to_string(index=False))\n",
    "print(f\"\\nTotal columns with missing values: {len(missing)}\")\n",
    "\n",
    "# Visualize missing values\n",
    "if len(missing) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.barh(missing['Column'], missing['Missing_Percentage'], color='coral')\n",
    "    ax.set_xlabel('Missing Percentage (%)', fontweight='bold')\n",
    "    ax.set_title('Missing Values by Column', fontsize=14, fontweight='bold', pad=20)\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (col, pct) in enumerate(zip(missing['Column'], missing['Missing_Percentage'])):\n",
    "        ax.text(pct, i, f' {pct:.1f}%', va='center', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 1: Drop columns with > 50% missing\n",
    "high_missing_cols = missing[missing['Missing_Percentage'] > 50]['Column'].tolist()\n",
    "print(f\"Columns to drop (>50% missing): {high_missing_cols}\")\n",
    "\n",
    "if high_missing_cols:\n",
    "    df_clean = df.drop(columns=high_missing_cols)\n",
    "    print(f\"Dropped {len(high_missing_cols)} columns\")\n",
    "else:\n",
    "    df_clean = df.copy()\n",
    "    print(\"No columns to drop\")\n",
    "\n",
    "# Strategy 2: Fill numeric missing with median\n",
    "numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
    "for col in numeric_cols:\n",
    "    if df_clean[col].isnull().sum() > 0:\n",
    "        median_val = df_clean[col].median()\n",
    "        df_clean[col].fillna(median_val, inplace=True)\n",
    "        print(f\"Filled {col} missing values with median: {median_val:,.0f}\")\n",
    "\n",
    "# Strategy 3: Fill categorical missing with 'Unknown'\n",
    "categorical_cols = df_clean.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    if df_clean[col].isnull().sum() > 0:\n",
    "        df_clean[col].fillna('Unknown', inplace=True)\n",
    "        print(f\"Filled {col} missing values with 'Unknown'\")\n",
    "\n",
    "# Verify\n",
    "print(f\"\\n✅ Missing values after cleaning: {df_clean.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Outlier Detection & Treatment\n",
    "\n",
    "### 4.1 Visualize Outliers - Pagu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check pagu outliers\n",
    "if 'pagu' in df_clean.columns:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Box plot\n",
    "    axes[0].boxplot(df_clean['pagu'] / 1_000_000, vert=True)\n",
    "    axes[0].set_ylabel('Pagu (Juta Rupiah)', fontweight='bold')\n",
    "    axes[0].set_title('Box Plot: Pagu (with outliers)', fontweight='bold')\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Histogram\n",
    "    axes[1].hist(df_clean['pagu'] / 1_000_000, bins=50, color='steelblue', edgecolor='black')\n",
    "    axes[1].set_xlabel('Pagu (Juta Rupiah)', fontweight='bold')\n",
    "    axes[1].set_ylabel('Frequency', fontweight='bold')\n",
    "    axes[1].set_title('Histogram: Pagu Distribution', fontweight='bold')\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistics\n",
    "    print(\"Pagu Statistics:\")\n",
    "    print(df_clean['pagu'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 IQR Method for Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers_iqr(data, column):\n",
    "    \"\"\"\n",
    "    Detect outliers using IQR method\n",
    "    \"\"\"\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
    "    \n",
    "    print(f\"Column: {column}\")\n",
    "    print(f\"Q1 (25th percentile): {Q1:,.0f}\")\n",
    "    print(f\"Q3 (75th percentile): {Q3:,.0f}\")\n",
    "    print(f\"IQR: {IQR:,.0f}\")\n",
    "    print(f\"Lower Bound: {lower_bound:,.0f}\")\n",
    "    print(f\"Upper Bound: {upper_bound:,.0f}\")\n",
    "    print(f\"Number of outliers: {len(outliers)} ({len(outliers)/len(data)*100:.2f}%)\")\n",
    "    \n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "# Detect outliers in pagu\n",
    "if 'pagu' in df_clean.columns:\n",
    "    outliers, lower, upper = detect_outliers_iqr(df_clean, 'pagu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Z-Score Method for Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers_zscore(data, column, threshold=3):\n",
    "    \"\"\"\n",
    "    Detect outliers using Z-score method\n",
    "    \"\"\"\n",
    "    z_scores = np.abs(stats.zscore(data[column].dropna()))\n",
    "    outliers_mask = z_scores > threshold\n",
    "    \n",
    "    print(f\"Column: {column}\")\n",
    "    print(f\"Z-score threshold: {threshold}\")\n",
    "    print(f\"Number of outliers: {outliers_mask.sum()} ({outliers_mask.sum()/len(data)*100:.2f}%)\")\n",
    "    \n",
    "    return outliers_mask\n",
    "\n",
    "# Detect with Z-score\n",
    "if 'pagu' in df_clean.columns:\n",
    "    zscore_outliers = detect_outliers_zscore(df_clean, 'pagu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Handle Outliers - Capping (Winsorization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Cap outliers using percentiles\n",
    "if 'pagu' in df_clean.columns:\n",
    "    df_capped = df_clean.copy()\n",
    "    \n",
    "    # Cap at 1st and 99th percentile\n",
    "    lower_cap = df_capped['pagu'].quantile(0.01)\n",
    "    upper_cap = df_capped['pagu'].quantile(0.99)\n",
    "    \n",
    "    df_capped['pagu_capped'] = df_capped['pagu'].clip(lower=lower_cap, upper=upper_cap)\n",
    "    \n",
    "    print(f\"Original pagu range: {df_clean['pagu'].min():,.0f} - {df_clean['pagu'].max():,.0f}\")\n",
    "    print(f\"Capped pagu range: {df_capped['pagu_capped'].min():,.0f} - {df_capped['pagu_capped'].max():,.0f}\")\n",
    "    \n",
    "    # Visualize before/after\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    axes[0].hist(df_clean['pagu'] / 1_000_000, bins=50, color='coral', edgecolor='black', alpha=0.7)\n",
    "    axes[0].set_title('Before Capping', fontweight='bold')\n",
    "    axes[0].set_xlabel('Pagu (Juta Rp)')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    \n",
    "    axes[1].hist(df_capped['pagu_capped'] / 1_000_000, bins=50, color='lightgreen', edgecolor='black', alpha=0.7)\n",
    "    axes[1].set_title('After Capping (1%-99%)', fontweight='bold')\n",
    "    axes[1].set_xlabel('Pagu (Juta Rp)')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Transformation\n",
    "\n",
    "### 5.1 Log Transformation for Skewed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'pagu' in df_clean.columns:\n",
    "    # Log transformation\n",
    "    df_clean['pagu_log'] = np.log1p(df_clean['pagu'])  # log1p = log(1+x) to handle zeros\n",
    "    \n",
    "    # Compare distributions\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Original\n",
    "    axes[0].hist(df_clean['pagu'] / 1_000_000, bins=50, color='steelblue', edgecolor='black')\n",
    "    axes[0].set_title(f'Original Distribution\\nSkewness: {df_clean[\"pagu\"].skew():.2f}', fontweight='bold')\n",
    "    axes[0].set_xlabel('Pagu (Juta Rp)')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    \n",
    "    # Log transformed\n",
    "    axes[1].hist(df_clean['pagu_log'], bins=50, color='coral', edgecolor='black')\n",
    "    axes[1].set_title(f'Log Transformed\\nSkewness: {df_clean[\"pagu_log\"].skew():.2f}', fontweight='bold')\n",
    "    axes[1].set_xlabel('Log(Pagu)')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Original skewness: {df_clean['pagu'].skew():.3f}\")\n",
    "    print(f\"Log transformed skewness: {df_clean['pagu_log'].skew():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Normalization & Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "if 'pagu' in df_clean.columns:\n",
    "    # Normalization (0-1)\n",
    "    scaler_minmax = MinMaxScaler()\n",
    "    df_clean['pagu_normalized'] = scaler_minmax.fit_transform(df_clean[['pagu']])\n",
    "    \n",
    "    # Standardization (mean=0, std=1)\n",
    "    scaler_standard = StandardScaler()\n",
    "    df_clean['pagu_standardized'] = scaler_standard.fit_transform(df_clean[['pagu']])\n",
    "    \n",
    "    # Compare\n",
    "    print(\"Original Pagu:\")\n",
    "    print(df_clean['pagu'].describe())\n",
    "    \n",
    "    print(\"\\nNormalized Pagu (0-1):\")\n",
    "    print(df_clean['pagu_normalized'].describe())\n",
    "    \n",
    "    print(\"\\nStandardized Pagu (mean=0, std=1):\")\n",
    "    print(df_clean['pagu_standardized'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Encoding Categorical Variables\n",
    "\n",
    "### 6.1 Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "if 'metode_pengadaan' in df_clean.columns:\n",
    "    # Label encoding\n",
    "    le = LabelEncoder()\n",
    "    df_clean['metode_encoded'] = le.fit_transform(df_clean['metode_pengadaan'])\n",
    "    \n",
    "    # Show mapping\n",
    "    mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "    print(\"Label Encoding Mapping:\")\n",
    "    for category, code in sorted(mapping.items(), key=lambda x: x[1]):\n",
    "        print(f\"  {code}: {category}\")\n",
    "    \n",
    "    # Compare\n",
    "    print(\"\\nSample comparison:\")\n",
    "    print(df_clean[['metode_pengadaan', 'metode_encoded']].drop_duplicates().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'metode_pengadaan' in df_clean.columns:\n",
    "    # Get top 5 methods to avoid too many columns\n",
    "    top_methods = df_clean['metode_pengadaan'].value_counts().head(5).index\n",
    "    df_subset = df_clean[df_clean['metode_pengadaan'].isin(top_methods)].copy()\n",
    "    \n",
    "    # One-hot encoding\n",
    "    df_onehot = pd.get_dummies(df_subset['metode_pengadaan'], prefix='metode')\n",
    "    \n",
    "    print(\"One-Hot Encoded columns:\")\n",
    "    print(df_onehot.columns.tolist())\n",
    "    \n",
    "    print(\"\\nSample:\")\n",
    "    print(df_onehot.head())\n",
    "    \n",
    "    print(f\"\\nOriginal shape: {df_subset.shape}\")\n",
    "    print(f\"After one-hot encoding: {pd.concat([df_subset, df_onehot], axis=1).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Frequency Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'metode_pengadaan' in df_clean.columns:\n",
    "    # Frequency encoding\n",
    "    freq_map = df_clean['metode_pengadaan'].value_counts().to_dict()\n",
    "    df_clean['metode_frequency'] = df_clean['metode_pengadaan'].map(freq_map)\n",
    "    \n",
    "    print(\"Frequency Encoding:\")\n",
    "    print(df_clean[['metode_pengadaan', 'metode_frequency']].drop_duplicates().sort_values('metode_frequency', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Engineering\n",
    "\n",
    "### 7.1 Date Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'tanggal_buat_paket' in df_clean.columns:\n",
    "    # Convert to datetime\n",
    "    df_clean['tanggal'] = pd.to_datetime(df_clean['tanggal_buat_paket'], errors='coerce')\n",
    "    \n",
    "    # Extract date features\n",
    "    df_clean['year'] = df_clean['tanggal'].dt.year\n",
    "    df_clean['month'] = df_clean['tanggal'].dt.month\n",
    "    df_clean['quarter'] = df_clean['tanggal'].dt.quarter\n",
    "    df_clean['day_of_week'] = df_clean['tanggal'].dt.dayofweek\n",
    "    df_clean['day_name'] = df_clean['tanggal'].dt.day_name()\n",
    "    df_clean['is_weekend'] = df_clean['day_of_week'].isin([5, 6]).astype(int)\n",
    "    df_clean['is_month_end'] = df_clean['tanggal'].dt.is_month_end.astype(int)\n",
    "    df_clean['is_quarter_end'] = df_clean['tanggal'].dt.is_quarter_end.astype(int)\n",
    "    df_clean['is_year_end'] = df_clean['tanggal'].dt.is_year_end.astype(int)\n",
    "    \n",
    "    print(\"Date Features Created:\")\n",
    "    date_features = ['tanggal', 'year', 'month', 'quarter', 'day_of_week', 'day_name', \n",
    "                     'is_weekend', 'is_month_end', 'is_quarter_end', 'is_year_end']\n",
    "    print(df_clean[date_features].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Binning - Create Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'pagu' in df_clean.columns:\n",
    "    # Create pagu categories\n",
    "    bins = [0, 100_000_000, 500_000_000, 1_000_000_000, 5_000_000_000, float('inf')]\n",
    "    labels = ['Sangat Kecil', 'Kecil', 'Sedang', 'Besar', 'Sangat Besar']\n",
    "    \n",
    "    df_clean['pagu_category'] = pd.cut(df_clean['pagu'], bins=bins, labels=labels)\n",
    "    \n",
    "    print(\"Pagu Categories:\")\n",
    "    print(df_clean['pagu_category'].value_counts().sort_index())\n",
    "    \n",
    "    # Visualize\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    df_clean['pagu_category'].value_counts().plot(kind='bar', ax=ax, color='steelblue', edgecolor='black')\n",
    "    ax.set_title('Distribusi Kategori Pagu', fontsize=14, fontweight='bold', pad=20)\n",
    "    ax.set_xlabel('Kategori Pagu', fontweight='bold')\n",
    "    ax.set_ylabel('Jumlah Paket', fontweight='bold')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Text Features (if applicable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there's a name/description column\n",
    "text_cols = [col for col in df_clean.columns if 'nama' in col.lower() or 'keterangan' in col.lower()]\n",
    "print(f\"Potential text columns: {text_cols}\")\n",
    "\n",
    "if 'nama_paket' in df_clean.columns:\n",
    "    # Text length\n",
    "    df_clean['nama_paket_length'] = df_clean['nama_paket'].str.len()\n",
    "    \n",
    "    # Word count\n",
    "    df_clean['nama_paket_word_count'] = df_clean['nama_paket'].str.split().str.len()\n",
    "    \n",
    "    print(\"Text Features:\")\n",
    "    print(df_clean[['nama_paket', 'nama_paket_length', 'nama_paket_word_count']].head())\n",
    "    \n",
    "    print(\"\\nText Statistics:\")\n",
    "    print(df_clean[['nama_paket_length', 'nama_paket_word_count']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Data Quality Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"DATA CLEANING SUMMARY REPORT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n1. DATASET SIZE\")\n",
    "print(f\"   Original: {df_original.shape}\")\n",
    "print(f\"   After Cleaning: {df_clean.shape}\")\n",
    "print(f\"   Rows removed: {len(df_original) - len(df_clean)}\")\n",
    "print(f\"   Columns added: {len(df_clean.columns) - len(df_original.columns)}\")\n",
    "\n",
    "print(f\"\\n2. MISSING VALUES\")\n",
    "print(f\"   Original: {df_original.isnull().sum().sum()}\")\n",
    "print(f\"   After Cleaning: {df_clean.isnull().sum().sum()}\")\n",
    "\n",
    "print(f\"\\n3. DUPLICATES\")\n",
    "print(f\"   Duplicate rows: {df_clean.duplicated().sum()}\")\n",
    "\n",
    "print(f\"\\n4. NEW FEATURES CREATED\")\n",
    "new_cols = set(df_clean.columns) - set(df_original.columns)\n",
    "for col in sorted(new_cols):\n",
    "    print(f\"   - {col}\")\n",
    "\n",
    "print(f\"\\n5. DATA TYPES\")\n",
    "print(df_clean.dtypes.value_counts())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = Path('../../../data')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Export cleaned data\n",
    "output_path = output_dir / 'rup_cleaned.parquet'\n",
    "df_clean.to_parquet(output_path, index=False)\n",
    "print(f\"✅ Cleaned data saved to: {output_path}\")\n",
    "print(f\"   File size: {output_path.stat().st_size / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Key Takeaways\n",
    "\n",
    "### Missing Values:\n",
    "- ✅ Drop columns with >50% missing\n",
    "- ✅ Fill numeric with median/mean\n",
    "- ✅ Fill categorical with mode/'Unknown'\n",
    "\n",
    "### Outliers:\n",
    "- ✅ IQR method for detection\n",
    "- ✅ Z-score method for detection\n",
    "- ✅ Capping/Winsorization for treatment\n",
    "- ✅ Log transformation for skewed data\n",
    "\n",
    "### Encoding:\n",
    "- ✅ Label Encoding untuk ordinal\n",
    "- ✅ One-Hot Encoding untuk nominal\n",
    "- ✅ Frequency Encoding untuk high cardinality\n",
    "\n",
    "### Feature Engineering:\n",
    "- ✅ Date features (year, month, quarter, etc.)\n",
    "- ✅ Binning untuk categories\n",
    "- ✅ Text features (length, word count)\n",
    "- ✅ Derived features dari existing data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
